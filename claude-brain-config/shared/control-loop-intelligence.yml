# Control Loop Intelligence System
# Automated iterative refinement for code generation tasks

Control_Loop_Execution:
  purpose: |
    Automated test-driven code generation with self-correction.
    Transforms SISO from interactive assistant to autonomous agent for testable tasks.

  activation_triggers:
    - Complex code generation requiring testing
    - Multi-file refactoring with validation
    - Bug fixes with automated test suites
    - Feature implementation with acceptance criteria
    - Any task with defined validation command

  loop_pattern: |
    1. GENERATE: Create initial solution using SISO intelligence
    2. VALIDATE: Run automated tests/checks
    3. OBSERVE: Capture results (success/errors)
    4. REFINE: If failed, analyze errors and regenerate
    5. REPEAT: Until success or max iterations

  implementation_flow:
    setup:
      - Read prd.yml for mission and validation criteria
      - Run install_command if specified
      - Prepare validation environment

    initial_generation:
      - Use mission and artifacts sections
      - Generate code files using current intelligence
      - Apply all SISO intelligence systems

    validation_loop:
      max_iterations: 5
      steps:
        - name: "Execute Validation"
          action: "Run test_command from prd.yml"
          success_condition: "Exit code 0"

        - name: "Capture Feedback"
          on_failure: "Capture stdout/stderr"
          on_success: "Terminate loop with success"

        - name: "Error Analysis"
          tools: ["zen-mcp debug", "zen-mcp codereview"]
          action: "Analyze failure root cause"

        - name: "Refinement"
          action: |
            Pass to LLM:
            - Original PRD mission
            - Current code
            - Validation error output
            - Error analysis
          prompt_template: |
            The following code failed validation. Analyze and fix:

            **PRD Mission:**
            {mission}

            **Current Code:**
            {code_files}

            **Validation Error:**
            {error_output}

            **Analysis:**
            {error_analysis}

            Provide corrected code that addresses the errors.

        - name: "Apply Fix"
          action: "Overwrite files with refined code"
          validation: "Syntax check before applying"

        - name: "Iterate"
          action: "Return to validation step"

    termination:
      success: "All validations pass"
      failure: "Max iterations reached"
      on_failure_action: "Preserve last attempt for manual review"

  validation_tools:
    code_execution:
      - Bash (compile/run)
      - testsprite (frontend testing)
      - npm/pytest/cargo test (language-specific)

    quality_checks:
      - zen-mcp codereview (code quality)
      - zen-mcp debug (error analysis)
      - linters/formatters

    custom_validation:
      - User-defined test commands in prd.yml
      - Health checks for services
      - API endpoint verification

  error_handling:
    permanent_errors:
      - Missing dependencies (suggest install)
      - Invalid PRD schema (halt with error)
      - Syntax errors in generated code (fix in next iteration)

    temporary_errors:
      - Network timeouts (retry)
      - Flaky tests (re-run validation)
      - Resource unavailable (wait and retry)

    retry_strategy:
      max_retries: 3
      backoff: exponential
      timeout: 60s per attempt

  prd_schema_requirement:
    format: YAML
    required_fields:
      - mission: "Human-readable task description"
      - environment: "Language, dependencies, install command"
      - artifacts: "Files to generate/modify"
      - validation: "test_command, run_command, manual_checks"
      - control_loop: "max_iterations"

    example: |
      # prd.yml
      mission: |
        Create a Node.js Express server with GET / returning {"status": "ok"}

      environment:
        language: "nodejs"
        dependencies: ["express"]
        install_command: "npm install"

      artifacts:
        - file: "server.js"
          description: "Main Express server"
        - file: "package.json"
          description: "Package manifest"

      validation:
        test_command: "npm test"
        run_command: "node server.js"
        manual_checks: |
          curl http://localhost:3000 should return {"status": "ok"}

      control_loop:
        max_iterations: 5

  integration_with_existing_systems:
    leverage_current_intelligence:
      - Use all SISO MCP tools for generation
      - Apply Musk's Algorithm at each step
      - Use zen-mcp tools for validation/analysis
      - Maintain session memory across iterations

    no_new_dependencies:
      - Orchestrates existing tools
      - No new infrastructure required
      - Simple YAML configuration

    measurement_metrics:
      - First-attempt success rate
      - Average iterations to success
      - Common error patterns
      - Token usage per iteration

  benefits:
    reliability:
      - Automated error detection and recovery
      - Systematic validation at each step
      - Reduces human QA cycles

    speed:
      - Faster iteration than manual debugging
      - Parallel validation possible
      - Immediate feedback to AI

    quality:
      - Code always meets validation criteria
      - Catches edge cases automatically
      - Consistent application of standards

    scalability:
      - Handles multiple tasks in sequence
      - Can run unattended
      - Reliable for complex multi-file changes

Codex_Loop_Command:
  name: "codex-loop"
  description: "Execute automated code generation with control loop"

  usage: |
    codex-loop [prd.yml]

    Reads prd.yml and executes:
    1. Setup environment (install dependencies)
    2. Generate initial code
    3. Run validation loop
    4. Output success/failure with metrics

  flags:
    --max-iterations: "Override default max iterations"
    --verbose: "Show detailed loop progress"
    --dry-run: "Generate without running validation"

  output:
    success: |
      ✓ Code generation successful
      - Iterations: 2/5
      - Validation: All tests passed
      - Files: server.js, package.json

    failure: |
      ✗ Code generation failed
      - Iterations: 5/5 (max reached)
      - Last error: [error output]
      - Files preserved for manual review

  implementation_notes:
    - Can be shell script or integrated into existing CLI
    - Logs each iteration for debugging
    - Preserves all attempts in .codex-loop/ directory
    - Supports rollback to any iteration

Control_Loop_Best_Practices:
  prd_design:
    - Make validation commands deterministic
    - Include both automated tests and manual checks
    - Specify exact success criteria
    - Keep max_iterations reasonable (3-5)

  prompt_engineering:
    - Include error context in refinement prompts
    - Show previous failed attempts
    - Ask for step-by-step reasoning
    - Request self-validation before applying

  monitoring:
    - Track iteration patterns
    - Identify common failure modes
    - Measure token efficiency
    - Update prompts based on patterns

  gradual_rollout:
    - Start with simple, well-tested tasks
    - Validate control loop on known-good PRDs
    - Expand to complex tasks incrementally
    - Keep human in loop for critical systems

Future_Enhancements:
  planned:
    - Parallel validation (run multiple checks simultaneously)
    - Adaptive max_iterations (based on task complexity)
    - Learning from past iterations (pattern recognition)
    - Multi-agent refinement (specialized agents per error type)

  deferred:
    - Persistent memory tiers (until clear use case)
    - Generalized tool orchestration (control loop is specialized)
    - Cross-project learning (start single-project first)
